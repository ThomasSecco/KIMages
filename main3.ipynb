{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pickle\n",
    "import ast\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, hamming_loss, jaccard_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import description,trad,functions\n",
    "from vect import simi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import classification,detection,yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification.eval('../Images/TN24_F243.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection.get_prediction('../Images/TN24_F252.jpg',0.5)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.yolo('../Images/TN24_F252.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for y in ['dog','chair','collie']:\n",
    "    for x in 'a dog sitting in a car seat with a leash'.split():\n",
    "        print(f'({x},{y}) : {round(simi(x,y),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(dog,collie) : 0.854')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=trad.final('excel.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('ready2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "desc = []\n",
    "\n",
    "try:\n",
    "    with open('desc_progress2.pkl', 'rb') as f:\n",
    "        desc = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "i = len(desc)\n",
    "for x in df['Bildbez'][len(desc):]:  \n",
    "    path = f'../Images/{x}.jpg'\n",
    "    try:\n",
    "        res = description.results(path)\n",
    "        c = description.compare_lists(res[1], res[2], res[3])\n",
    "        f = description.score(c[0], c[1], c[2], res[0])\n",
    "        if f[1] > 50:\n",
    "            detail = description.details(path, res)\n",
    "            a = [f[0]] + detail\n",
    "            desc.append(a)\n",
    "        else:\n",
    "            desc.append([''])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        desc.append([''])\n",
    "\n",
    "\n",
    "    with open('desc_progress2.pkl', 'wb') as f:\n",
    "        pickle.dump(desc, f)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    print(f'Progess : {100*i/df.shape[0]}%')\n",
    "\n",
    "df['Description']=desc\n",
    "df.to_excel('ready2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel('excel.xlsx')\n",
    "df['class']=df1['Hauptoberkategorie']\n",
    "\n",
    "def convert_to_list(value):\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return value\n",
    "\n",
    "df['Description']=df['Description'].apply(convert_to_list)  #When loading a excel file, all values will be strings so we need to convert them back to list\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in ['Photo scene','Photo title','Note','Description']:   \n",
    "        df[col]=df[col].fillna('')\n",
    "    else:\n",
    "        df[col]=df[col].fillna(0)\n",
    "\n",
    "\n",
    "y_true=df.drop(columns=['Bildbez','Photo scene','Photo title','Note','class','Description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=x=[df['Photo scene'],df['Photo title'],df['Note']]\n",
    "df=df.drop(columns=['Photo scene','Photo title','Note'])\n",
    "\n",
    "a=[]\n",
    "for i in x[0]:\n",
    "    numbers = re.findall(r'-?\\d+\\.?\\d*', i)\n",
    "    a.append([int(num) if num.isdigit() else float(num) for num in numbers])\n",
    "\n",
    "\n",
    "b=[]\n",
    "for i in x[1]:\n",
    "    if type(i)==str:\n",
    "        numbers = re.findall(r'-?\\d+\\.?\\d*', i)\n",
    "        b.append([int(num) if num.isdigit() else float(num) for num in numbers])\n",
    "    else:\n",
    "        b.append(i)\n",
    "\n",
    "c=[]\n",
    "for i in x[2]:\n",
    "    if type(i)==str:\n",
    "        numbers = re.findall(r'-?\\d+\\.?\\d*', i)\n",
    "        c.append([int(num) if num.isdigit() else float(num) for num in numbers])\n",
    "    else:\n",
    "        c.append(i)\n",
    "\n",
    "        \n",
    "df['Photo motif']=a\n",
    "df['Photo title']=b\n",
    "df['Note']=c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark=functions.benchmark(df)\n",
    "y_pred=functions.prediction_list(df[['Photo scene','Photo title','Note','Description']],benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pd.read_csv('y_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "# Set PyTorch to be deterministic\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Concatenate sentence columns into a single text field\n",
    "df_text = df['Photo scene'] + ' ' + df['Photo title'] + ' ' + df['Note'].fillna('') + ' ' + df['Description'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Extract labels\n",
    "label=df.drop(columns=['Bildbez','Photo scene','Photo title','Note','class','Description'])\n",
    "label_columns = label.columns\n",
    "df_labels = df[label_columns].values\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df_text, df_labels, test_size=0.2, random_state=42)\n",
    "# Convert numpy arrays to torch tensors\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.float32)\n",
    "all_labels_tensor = torch.tensor(df_labels, dtype=torch.float32)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',cache_dir='c:/Stage/cache')\n",
    "\n",
    "# Tokenize dataset\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "all_text=tokenizer(df_text.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convert to Dataset format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': train_labels_tensor\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'attention_mask': val_encodings['attention_mask'],\n",
    "    'labels': val_labels_tensor\n",
    "})\n",
    "\n",
    "all_dataset=Dataset.from_dict({\n",
    "    'input_ids': all_text['input_ids'],\n",
    "    'attention_mask': all_text['attention_mask'],\n",
    "    'labels': all_labels_tensor\n",
    "})\n",
    "\n",
    "sub=['1_1','1_2','1_3','1_4','1_5','2_1','2_2','2_3','3_1','4_3','5_1','5_2','5_3','5_4','5_5','6_1','6_2','6_3','6_4','6_5','4_4','4_1','3_2']\n",
    "sub_cat=dict(zip(label_columns,sub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "probs=[]\n",
    "benchs=[round(0.1*i,2) for i in range(1,10)]\n",
    "\n",
    "\n",
    "for x in [0.25,0.3,0.35]:\n",
    "    for learn_rate in [2e-4,2e-3]:\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_columns), problem_type=\"multi_label_classification\")\n",
    "\n",
    "        # Define training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            evaluation_strategy='epoch',\n",
    "            save_strategy='epoch',  \n",
    "            learning_rate=learn_rate,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            save_total_limit=1,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\"\n",
    "        )\n",
    "\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "        def compute_metrics(eval_pred, threshold):\n",
    "            predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "            predictions = torch.sigmoid(torch.tensor(predictions))  # Apply sigmoid to get probabilities \n",
    "            pred_labels = (predictions > threshold).int().numpy()  # Convert probabilities to binary labels\n",
    "            labels = labels.astype(int)  # Ensure labels are also in integer format\n",
    "            f1 = f1_score(labels, pred_labels, average='macro', zero_division=1)\n",
    "            \n",
    "            return {\"eval_f1\": f1}\n",
    "\n",
    "\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=lambda p: compute_metrics(p, x)\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate the model\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = trainer.predict(all_dataset)\n",
    "        pred_probs = torch.sigmoid(torch.tensor(predictions.predictions))\n",
    "        pred_labels = (pred_probs > x).int()\n",
    "        probs.append(pred_probs)\n",
    "        preds.append(pred_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df={}\n",
    "probs_df={}\n",
    "for i in range (1,len(preds)+1):\n",
    "    preds_df[f'y_pred{i}']=pd.DataFrame(preds[i-1],columns=label_columns,index=df['Bildbez'])\n",
    "    probs_df[f'y_prob{i}']=pd.DataFrame(probs[i-1],columns=label_columns,index=df['Bildbez'])\n",
    "    \n",
    "\n",
    "\n",
    "with open('bert_preds10.pkl', 'wb') as file:\n",
    "    pickle.dump(preds_df, file)\n",
    "\n",
    "with open('bert_probs10.pkl', 'wb') as file:\n",
    "    pickle.dump(probs_df, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_preds.pkl', 'rb') as file:\n",
    "    preds_df = pickle.load(file)\n",
    "\n",
    "with open('bert_probs.pkl', 'rb') as file:\n",
    "    probs_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]  # % of true labels in the predicted labels/ % of true labels predicted out of all the true labels\n",
    "for x in preds_df:\n",
    "    t=0\n",
    "    result=[]\n",
    "    for i in range(preds_df[x].shape[0]):\n",
    "        pred_row=preds_df[x].iloc[i]\n",
    "        true_row=y_true.iloc[i]\n",
    "        pred_labels=[col for col in pred_row.index if pred_row[col]==1]\n",
    "        true_labels=[col for col in true_row.index if true_row[col]==1]\n",
    "        c=0\n",
    "        for y in pred_labels:\n",
    "            if y in true_labels:\n",
    "                c+=1\n",
    "        if len(pred_labels)!=0:\n",
    "            result.append([c/len(pred_labels),c/len(true_labels)])\n",
    "            if c/len(pred_labels)+c/len(true_labels)>1.2:\n",
    "                t+=1\n",
    "        else:\n",
    "            result.append([0,0])\n",
    "    print(t)        \n",
    "    results.append(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Bildbez']=='TN2_F62']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]  # % of true labels in the predicted labels/ % of true labels predicted out of all the true labels\n",
    "x='y_pred2'\n",
    "t=0\n",
    "result=[]\n",
    "for i in range(preds_df[x].shape[0]):\n",
    "    pred_row=preds_df[x].iloc[i]\n",
    "    true_row=y_true.iloc[i]\n",
    "    pred_labels=[col for col in pred_row.index if pred_row[col]==1]\n",
    "    true_labels=[col for col in true_row.index if true_row[col]==1]\n",
    "    c=0\n",
    "    for y in pred_labels:\n",
    "        if y in true_labels:\n",
    "            c+=1\n",
    "    if len(pred_labels)!=0:\n",
    "        if c/len(pred_labels)+c/len(true_labels)>=2:\n",
    "            a=df['Bildbez'][i]\n",
    "            display(Image.open(f'../Images/{a}.jpg'))\n",
    "            print('True labels : ',true_labels)\n",
    "            print('Predicted labels : ',pred_labels)\n",
    "            print(c/len(true_labels),c/len(pred_labels))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench=[0.5+i/10 for i in range(16)]\n",
    "\n",
    "for name in range (1,11):\n",
    "    goods=[]\n",
    "    if name==1:\n",
    "        name=''\n",
    "    with open(f'bert_preds{name}.pkl', 'rb') as file:\n",
    "        preds_df = pickle.load(file)\n",
    "    for j in range (len(bench)):\n",
    "        good=[]\n",
    "        for x in preds_df:\n",
    "            t=0\n",
    "            for i in range(preds_df[x].shape[0]):\n",
    "                pred_row=preds_df[x].iloc[i]\n",
    "                true_row=y_true.iloc[i]\n",
    "                pred_labels=[col for col in pred_row.index if pred_row[col]==1]\n",
    "                true_labels=[col for col in true_row.index if true_row[col]==1]\n",
    "                c=0\n",
    "                for y in pred_labels:\n",
    "                    if y in true_labels:\n",
    "                        c+=1\n",
    "                if len(pred_labels)!=0:\n",
    "                    test=c/len(pred_labels)+c/len(true_labels)\n",
    "                    if j!=len(bench)-1:\n",
    "                        if test>=bench[j] and test<bench[j+1]:\n",
    "                            t+=1\n",
    "                    else:\n",
    "                        if test>=bench[j]:\n",
    "                            t+=1\n",
    "            good.append(t)\n",
    "        goods.append(good)\n",
    "    goods=pd.DataFrame(goods,index=bench)\n",
    "    print(goods)\n",
    "    print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(picture : str, model : int, sub_cat : dict):\n",
    "    row=probs_df[f'y_prob{model}'].loc[picture].copy()\n",
    "    main_cat=[]\n",
    "    sub_cats=[]\n",
    "    while True:\n",
    "        max_prob=row.max()\n",
    "        label=row.idxmax()\n",
    "        sub=sub_cat[label]\n",
    "        if sub[0] not in main_cat and max_prob>=model/10:\n",
    "            main_cat.append(sub[0])\n",
    "            sub_cats.append(label)\n",
    "            row[label]=0\n",
    "        elif sub[0] in main_cat:\n",
    "            row[label]=0\n",
    "        elif max_prob<model/10:\n",
    "            break\n",
    "    main=main_cat[0]\n",
    "    print(f'The main categorie is {main}')\n",
    "    print('The sub-catefogories are :')\n",
    "    for x in sub_cats:\n",
    "        print(x,sub_cat[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred('TN1_F96',4,sub_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_row=preds_df['y_pred1'].iloc[231]\n",
    "true_row=y_true.iloc[231]\n",
    "pred_labels=[true_row.index[col] for col in pred_row.index if pred_row[col]==1]\n",
    "true_labels=[col for col in true_row.index if true_row[col]==1]\n",
    "print(f'The true labels are : {true_labels}')\n",
    "print()\n",
    "print()\n",
    "print(f'The predicted labels are : {pred_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "result=[]\n",
    "for i in range(y_pred.shape[0]):\n",
    "    pred_row=preds_df['y_pred2'].iloc[i]\n",
    "    true_row=y_true.iloc[i]\n",
    "    pred_labels=[true_row.index[col] for col in pred_row.index if pred_row[col]==1]\n",
    "    true_labels=[col for col in true_row.index if true_row[col]==1]\n",
    "    c=0\n",
    "    lab=[]\n",
    "    for y in pred_labels:\n",
    "        if y in true_labels:\n",
    "            lab.append(y)\n",
    "            c+=1\n",
    "    if len(pred_labels)!=0:\n",
    "        result.append([c/len(pred_labels),c/len(true_labels)])\n",
    "        if c/len(pred_labels)+c/len(true_labels)>1.2:\n",
    "            t+=1\n",
    "            print(c/len(pred_labels),c/len(true_labels))\n",
    "\n",
    "    else:\n",
    "        result.append([0,0])\n",
    "print(t)        \n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=[]\n",
    "for i in range (y_true.shape[0]):\n",
    "    prob=[]\n",
    "    true_row=y_true.iloc[i]\n",
    "    prob_row=pred_probs[i]\n",
    "    for j,col in enumerate(y_true.columns):\n",
    "        if true_row[col]==1:\n",
    "            prob.append(float(prob_row[j]))\n",
    "        else:\n",
    "            prob.append(0)\n",
    "    probs.append(prob)\n",
    "\n",
    "print(pd.DataFrame(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "result=[]\n",
    "for i in range(y_pred.shape[0]):\n",
    "    pred_row=y_pred.iloc[i]\n",
    "    true_row=y_true.iloc[i]\n",
    "    pred_labels=[col for col in pred_row.index if pred_row[col]==1]\n",
    "    true_labels=[col for col in true_row.index if true_row[col]==1]\n",
    "    c=0\n",
    "    for y in pred_labels:\n",
    "        if y in true_labels:\n",
    "            c+=1\n",
    "    if len(pred_labels)!=0:\n",
    "        result.append([c/len(pred_labels),c/len(true_labels)])\n",
    "        if c/len(pred_labels)+c/len(true_labels)>1.2:\n",
    "            t+=1\n",
    "            print(c/len(pred_labels),c/len(true_labels))\n",
    "    else:\n",
    "        result.append([0,0])\n",
    "print(t)        \n",
    "results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(results)):\n",
    "    to_plot=pd.DataFrame(results[i])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    to_plot.boxplot(column=[0,1])\n",
    "    plt.title('Boxplots of Value1 and Value2')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy =  []\n",
    "precision = [] \n",
    "recall= [] \n",
    "f1 = []  \n",
    "roc_auc = []\n",
    "hamming = []\n",
    "jaccard = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(y_test_true.columns)):\n",
    "    y_pred1=y_test_pred[str(i)]\n",
    "    y_true1=y_tes[i]\n",
    "\n",
    "    accuracy.append(accuracy_score(y_true1, y_pred1))\n",
    "    precision.append(precision_score(y_true1, y_pred1, average='micro')  )\n",
    "    recall.append(recall_score(y_true1, y_pred1, average='micro')  )\n",
    "    f1.append(f1_score(y_true1, y_pred1, average='micro')  )\n",
    "    try:\n",
    "        roc_auc.append(roc_auc_score(y_true1, y_pred1, average='micro')  )\n",
    "    except ValueError:\n",
    "        roc_auc.append(0.5)\n",
    "    hamming.append(hamming_loss(y_true1, y_pred1))\n",
    "    jaccard.append(jaccard_score(y_true1, y_pred1, average='micro'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = [accuracy, precision, recall, f1, roc_auc, hamming, jaccard]\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Hamming Loss', 'Jaccard Score']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(data, labels=labels)\n",
    "plt.title(f'Boxplot')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy =  []\n",
    "precision = [] \n",
    "recall= [] \n",
    "f1 = []  \n",
    "roc_auc = []\n",
    "hamming = []\n",
    "jaccard = []\n",
    "\n",
    "\n",
    "\n",
    "for x in y_true.columns:\n",
    "    y_true1=y_true[x]\n",
    "    y_pred1=y_pred[x]\n",
    "\n",
    "    accuracy.append(accuracy_score(y_true1, y_pred1))\n",
    "    precision.append(precision_score(y_true1, y_pred1, average='micro')  )\n",
    "    recall.append(recall_score(y_true1, y_pred1, average='micro')  )\n",
    "    f1.append(f1_score(y_true1, y_pred1, average='micro')  )\n",
    "    roc_auc.append(roc_auc_score(y_true1, y_pred1, average='micro')  )\n",
    "    hamming.append(hamming_loss(y_true1, y_pred1))\n",
    "    jaccard.append(jaccard_score(y_true1, y_pred1, average='micro'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = [accuracy, precision, recall, f1, roc_auc, hamming, jaccard]\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Hamming Loss', 'Jaccard Score']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(data, labels=labels)\n",
    "plt.title(f'Boxplot')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "desc = []\n",
    "i = 0\n",
    "try:\n",
    "    with open('desc_progress.pkl', 'rb') as f:\n",
    "        desc = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "for x in df['Bildbez'][len(desc):]:  \n",
    "    print(i)\n",
    "    path = f'../Images/{x}.jpg'\n",
    "    try:\n",
    "        res = description.results(path)\n",
    "        c = description.compare_lists(res[1], res[2], res[3])\n",
    "        f = description.score(c[0], c[1], c[2], res[0])\n",
    "        if f[1] > 0.5:\n",
    "            detail = description.details(path, res)\n",
    "            a = [f[0]]\n",
    "            for y in detail:\n",
    "                a.append(y)\n",
    "            desc.append(a)\n",
    "        else:\n",
    "            desc.append([''])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        desc.append([''])\n",
    "\n",
    "\n",
    "    with open('desc_progress.pkl', 'wb') as f:\n",
    "        pickle.dump(desc, f)\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "desc_vec=[]\n",
    "for y in desc:\n",
    "    if type(y)==list:\n",
    "        a=[]\n",
    "        for x in y:\n",
    "            a.append(nlp(x).vector)\n",
    "        desc_vec.append(a)\n",
    "    else:\n",
    "        desc_vec.append(nlp(y).vector)\n",
    "\n",
    "df['description']=desc_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
